{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791e6294",
   "metadata": {},
   "source": [
    "## Building A ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacde8af",
   "metadata": {},
   "source": [
    "How to design and implement an LLM-powered chatbot.This chatbot will be able to have a conversation and remember previous interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30718ccd",
   "metadata": {},
   "source": [
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069b2b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571660af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading API Key\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188ddd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Chatbot_With_Message_History_Using_Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000255408238C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000025525C786E0>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading model\n",
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(groq_api_key=groq_api_key,model_name='openai/gpt-oss-20b')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Krish! 👋 It’s great to meet the chief AI engineer. How can I assist you today? Whether it’s brainstorming, debugging, or exploring new AI concepts, I’m here to help.', additional_kwargs={'reasoning_content': 'We should respond with a friendly greeting, mention that we are an AI assistant. The user says \"Hey hi, my name is Krish and I am chief AI engineer.\" We should acknowledge. Maybe ask how we can help.'}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 86, 'total_tokens': 184, 'completion_time': 0.10438541, 'prompt_time': 0.004397059, 'queue_time': 0.043167361, 'total_time': 0.108782469}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8fa9509a-c94b-4b40-93ef-e4a9bf7663d4-0', usage_metadata={'input_tokens': 86, 'output_tokens': 98, 'total_tokens': 184})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "# User is giving the message\n",
    "model.invoke([HumanMessage(content=\"Hey hi, my name is Krish and I am chief AI engineer.\")])\n",
    "# Here we have get the response from the AI from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d1f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You’re Krish, and you’re a Chief AI Engineer. In that role, you’re responsible for overseeing AI strategy, guiding teams on model development, ensuring ethical AI practices, and driving innovation in AI products and services.' additional_kwargs={'reasoning_content': 'The user says: \"Hey, what\\'s my name and what do I do?\" They previously said: \"Hey, my name is Krish and I am a chief AI engineer.\" So the assistant should answer: name is Krish, they are a chief AI engineer. Also maybe elaborate. Ensure no policy violation. It\\'s straightforward.'} response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 151, 'total_tokens': 272, 'completion_time': 0.122062419, 'prompt_time': 0.007431785, 'queue_time': 0.042793915, 'total_time': 0.129494204}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--7d599c8d-411c-4372-88eb-9cac41f8fa39-0' usage_metadata={'input_tokens': 151, 'output_tokens': 121, 'total_tokens': 272}\n"
     ]
    }
   ],
   "source": [
    "# Creating the sequence of messages\n",
    "from langchain_core.messages import AIMessage\n",
    "messages = (\n",
    "    [\n",
    "        HumanMessage(content=\"Hey, my name is Krish and I am a chief AI engineer.\"),\n",
    "        AIMessage(content=\"Hello, Krish. It's nice to meet you as a chief engineer. What kind of projects are you working on these days? I am always eager to learn about the exciting work being done in the field of AI.\"),\n",
    "        HumanMessage(content=\"Hey, what's my name and what do I do?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f908a2",
   "metadata": {},
   "source": [
    "# Message History (How can the model remember the past conversation History)\n",
    "Message history is essential for maintaining context across different sessions and users. In real-world applications, multiple sessions may occur simultaneously, and each session must retain its own context. This is managed using session IDs and message history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4865cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d727202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Session-Based Message History\n",
    "# We create a function to manage session-based message history. Each session ID corresponds to a separate chat history, stored in a dictionary.\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory :\n",
    "    # Session id is use to distinguish between the differenrt sessions\n",
    "    if session_id not in store :\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04033f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c95200af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you again, Krish! How can I help you today? Whether you’re working on a new model, need code snippets, or just want to brainstorm ideas, feel free to let me know.\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Hi, my name is Krish and I am an AI engineer.\")], config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6565ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Krish.\n"
     ]
    }
   ],
   "source": [
    "response1 = with_message_history.invoke([HumanMessage(content=\"What's is my name\")],config=config)\n",
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b295d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the config\n",
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d5e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don’t have that information unless you share it with me. What would you like me to call you?\n"
     ]
    }
   ],
   "source": [
    "response1 = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name\")\n",
    "],config=config1)\n",
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0a2cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, John! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response2 = with_message_history.invoke([HumanMessage(content=\"My name is John\")],config=config1)\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is John.\n"
     ]
    }
   ],
   "source": [
    "response2 = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name\")\n",
    "],config=config1)\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edc02b",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "Prompt templates help turn raw user information into a format that a language model can work with. If the raw user input is just a message, we can use prompt templates to structure it before passing it to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f33869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your availability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "325c515f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Krish! 👋 How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hii My name is Krish\". They didn\\'t ask a question. We can respond with a friendly greeting, maybe ask how we can help.'}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 97, 'total_tokens': 153, 'completion_time': 0.055791694, 'prompt_time': 0.004706613, 'queue_time': 0.042591473, 'total_time': 0.060498307}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a007abb-3199-4abe-a0b6-2bf8020a50f2-0', usage_metadata={'input_tokens': 97, 'output_tokens': 56, 'total_tokens': 153})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoking the chain\n",
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hii My name is Krish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a90ef796",
   "metadata": {},
   "outputs": [],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d99a8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Krish! 👋 How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)\n",
    "config={\"configurations\":{\"session_id\":\"chat3\"}}\n",
    "response =  with_message_history.invoke([HumanMessage(content=\"Hii My name is Krish\")],config=config3)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b87bb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते, कृष्ण जी! आप कैसे हैं? मैं आपकी किस प्रकार सहायता कर सकता हूँ?\n"
     ]
    }
   ],
   "source": [
    "## Multile Input (Adding more complexity)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all the questions to your ability in this language: {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chain = chat_prompt|model\n",
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hii My name is Krish\")],\"language\":\"Hindi\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c151c71",
   "metadata": {},
   "source": [
    "# Managing Chat History with Multiple Keys\n",
    "When using multiple input variables, specify the correct input message key in the message history class. This ensures the chat history is saved and retrieved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71edeb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते, कृष्ण जी! आपसे मिलकर खुशी हुई। मैं आपकी किस प्रकार सहायता कर सकता हूँ?\n"
     ]
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hii,I am Krish\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82891b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आपका नाम कृष्ण है।\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What's my name?\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9b09599",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are the language translator and your  work is to translate the input text from {language1} to {language2}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "708f9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_translator_chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    language_translator_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57988570",
   "metadata": {},
   "outputs": [],
   "source": [
    "config5={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d97281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":[HumanMessage(content=\"What are you doing ?\")], # Input Given by user\n",
    "        \"language1\":\"English\",\n",
    "        \"language2\":\"Hindi\"\n",
    "    },\n",
    "    config=config5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae0669cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आप क्या कर रहे हैं?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e7f46",
   "metadata": {},
   "source": [
    "# Manage the Conversational History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e902a",
   "metadata": {},
   "source": [
    "The focus is on managing the conversation history when building a chatbot using Langchain. Proper management of conversation history is crucial, as an unmanaged list of messages can grow unbounded and potentially overflow the context window of the language model (LM). Therefore, it is important to add a step that limits the size of the messages being passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0492cc",
   "metadata": {},
   "source": [
    "# Trim_Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0c8c9",
   "metadata": {},
   "source": [
    "The trim_messages helper is used to reduce the number of messages sent to the model. It allows specifying how many tokens to keep and provides other parameters such as whether to always keep the system message and whether to allow partial messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73963bdf",
   "metadata": {},
   "source": [
    "The system message guides the AI’s behavior.\n",
    "\n",
    "The human message provides input.\n",
    "\n",
    "The AI message records output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d45a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hey, I am Bob.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is two plus two?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Four.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "# Set of conversation between Human and AI\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a good assistant.\"),\n",
    "    HumanMessage(content=\"Hey, I am Bob.\"),\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream.\"),\n",
    "    AIMessage(content=\"Nice.\"),\n",
    "    HumanMessage(content=\"What is two plus two?\"),\n",
    "    AIMessage(content=\"Four.\"),\n",
    "    HumanMessage(content=\"Thanks.\")\n",
    "]\n",
    "\n",
    "trimmed = trimmer.invoke(messages)\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b2a01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your availability in {language} language\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d2d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You mentioned that you like vanilla ice cream.\n"
     ]
    }
   ],
   "source": [
    "# How to pass the trimmer in chain\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    |prompt\n",
    "    |model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages+[HumanMessage(content=\"What icecream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe5765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: “What is two plus two?”\n"
     ]
    }
   ],
   "source": [
    "response1 = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages+[HumanMessage(content=\"What math problem did I ask?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ddf873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four.\n"
     ]
    }
   ],
   "source": [
    "response2 = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages+[HumanMessage(content=\"What is the answer of two plus two\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13d277f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's wrap in the Messsage History\n",
    "with_message_history =  RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "config5={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 4.\n"
     ]
    }
   ],
   "source": [
    "response2 = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages+[HumanMessage(content=\"What is the answer of two plus two\")], # Here we are define the old messaged with the new messages\n",
    "    \"language\":\"English\"\n",
    "    },\n",
    "    config=config5\n",
    ")\n",
    "print(response2.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
